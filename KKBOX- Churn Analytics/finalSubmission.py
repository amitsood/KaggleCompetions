#Import libraries
from multiprocessing import Pool, cpu_count
import gc; gc.enable()
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn import *
import sklearn
import os
#Set directory path
os.chdir("/Users/amit.sood/Documents/Analytics/kaggle/ChurnAnalysis/");


#Load data
#There are 2 train datasets available. Lets merge them
train = pd.read_csv('../ChurnAnalysis/train.csv')
train = pd.concat((train, pd.read_csv('../ChurnAnalysis/train_v2.csv')), axis=0, ignore_index=True).reset_index(drop=True)
test = pd.read_csv('../ChurnAnalysis/sample_submission_v2.csv')


#Feature Engineering

#Feature - Total transaction cout per user(trans_count)
transactions = pd.read_csv('../ChurnAnalysis/transactions.csv', usecols=['msno'])
transactions = pd.concat((transactions, pd.read_csv('../ChurnAnalysis/transactions_v2.csv', usecols=['msno'])), axis=0, ignore_index=True).reset_index(drop=True)
transactions = pd.DataFrame(transactions['msno'].value_counts().reset_index())
transactions.columns = ['msno','trans_count']


#Merge transactions feature to train and test data
train = pd.merge(train, transactions, how='left', on='msno')
test = pd.merge(test, transactions, how='left', on='msno')
#free transactions object
transactions = []; 


#Feature - Total logs generated by user(logs_count)
user_logs = pd.read_csv('../ChurnAnalysis/user_logs_v2.csv', usecols=['msno'])
user_logs = pd.DataFrame(user_logs['msno'].value_counts().reset_index())
user_logs.columns = ['msno','logs_count']

#Merge logs count to train and test data
train = pd.merge(train, user_logs, how='left', on='msno')
test = pd.merge(test, user_logs, how='left', on='msno')
# free user_logs object
user_logs = []; 


#Load user demographics data
members = pd.read_csv('../ChurnAnalysis/members_v3.csv')

#Merge user demographics into training and test data
train = pd.merge(train, members, how='left', on='msno')
test = pd.merge(test, members, how='left', on='msno')
members = [];

#Encode gender 
gender = {'male':1, 'female':2}
train['gender'] = train['gender'].map(gender)
test['gender'] = test['gender'].map(gender)



#Merge user transaction data to train data
transactions = pd.read_csv('../ChurnAnalysis/transactions_v2.csv') 
transactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)
transactions = transactions.drop_duplicates(subset=['msno'], keep='first')

train = pd.merge(train, transactions, how='left', on='msno')
test = pd.merge(test, transactions, how='left', on='msno')
transactions=[]


def transform_df(df):
    df = pd.DataFrame(df)
    df = df.sort_values(by=['date'], ascending=[False])
    df = df.reset_index(drop=True)
    df = df.drop_duplicates(subset=['msno'], keep='first')
    return df

def transform_df2(df):
    df = df.sort_values(by=['date'], ascending=[False])
    df = df.reset_index(drop=True)
    df = df.drop_duplicates(subset=['msno'], keep='first')
    return df

df_iter = pd.read_csv('../ChurnAnalysis/user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)
last_user_logs = []
last_user_logs.append(transform_df(pd.read_csv('../ChurnAnalysis/user_logs_v2.csv')))
last_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)
last_user_logs = transform_df2(last_user_logs)



train = pd.merge(train, last_user_logs, how='left', on='msno')
test = pd.merge(test, last_user_logs, how='left', on='msno')
last_user_logs=[]


train = train.fillna(0)
test = test.fillna(0)

#train.to_csv('xgbDataTrain.csv',index=False)
#test.to_csv('xgbDataTest.csv',index=False)

#Save the data so that next time we can load it directly and save some time
train = pd.read_csv('../ChurnAnalysis/xgbDataTrain.csv')
test = pd.read_csv('../ChurnAnalysis/xgbDataTest.csv')

cols = [c for c in train.columns if c not in ['is_churn','msno']]

# *********************** XGBoost starts
def xgb_score(preds, dtrain):
    labels = dtrain.get_label()
    return 'log_loss', metrics.log_loss(labels, preds)

fold = 3
for i in range(fold):
    params = {
        'eta': 1,
        'max_depth': 3,
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',
        'seed': i,
        'silent': True
    }
    x1, x2, y1, y2 = model_selection.train_test_split(train[cols], train['is_churn'], test_size=0.3, random_state=i)
    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]
    model = xgb.train(params, xgb.DMatrix(x1, y1), 300,  watchlist, feval=xgb_score, maximize=False, verbose_eval=50, early_stopping_rounds=10) 
    if i != 0:
        pred += model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)
    else:
        pred = model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)


pred /= fold
test['is_churn'] = pred.clip(0.+1e-15, 1-1e-15)
test[['msno','is_churn']].to_csv('submission_xgb.csv', index=False)
# *********************** XGBoost ends

# *********************** KNeighborsClassifier starts
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation  import train_test_split
from sklearn.cross_validation  import cross_val_score
X_train, X_test, y_train, y_test = train_test_split(train[cols], train['is_churn'], random_state=1)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,y_train)
y_pred= knn.predict(X_test)
print(metrics.accuracy_score(y_test,y_pred))

test['is_churn']= knn.predict(test[cols])
test[['msno','is_churn']].to_csv('submission_Knn.csv', index=False)
# *********************** KNeighborsClassifier ends

# *********************** NaiveBayes starts
import numpy as np
X = np.random.randint(2, size=(6, 100))
Y = np.array([1, 2, 3, 4, 4, 5])
from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()
clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
print(clf.predict(X[2:3]))

# *********************** NaiveBayes ends


#*********************Parameter tunning ****************************
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
data = pd.read_csv('../ChurnAnalysis/xgbDataTrain.csv')
cols = [c for c in data.columns if c not in ['is_churn','msno']]
x1, x2, y1, y2 = model_selection.train_test_split(data[cols], data['is_churn'], test_size=0.3, random_state=1)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y1)


# grid search
model = XGBClassifier()
n_estimators = [100, 200, 300]
max_depth = [3, 5, 7]
param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)
#param_grid = dict(max_depth=max_depth)
kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold, verbose=1)
grid_result = grid_search.fit(x1, label_encoded_y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
	print("%f (%f) with: %r" % (mean, stdev, param))
# plot results
scores = numpy.array(means).reshape(len(max_depth), len(n_estimators))
for i, value in enumerate(max_depth):
    pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value))
pyplot.legend()
pyplot.xlabel('n_estimators')
pyplot.ylabel('Log Loss')
pyplot.savefig('n_estimators_vs_max_depth.png')






